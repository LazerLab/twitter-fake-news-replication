{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "import io\n",
    "import tldextract\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import cPickle as pickle\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_url_re = re.compile(\"(snopes.com|photo|.jpg|.png)\",re.I|re.U)\n",
    "\n",
    "def parse_article(dat):\n",
    "    i,filename = dat\n",
    "    if i % 250 == 0:\n",
    "        print i\n",
    "    \n",
    "    page_html = io.open(filename).read()\n",
    "    soup = BeautifulSoup(page_html,\"lxml\")\n",
    "    article_text = soup.find(\"div\",{\"class\":\"article-text\"})\n",
    "    \n",
    "    \n",
    "    if not article_text:\n",
    "        #print 'No article text!!! ', filename\n",
    "        return {\"article_url\": filename,\n",
    "                \"parse_result\" : \"no_article_text\"}\n",
    "    \n",
    "    ############## URL STUFF ###########################\n",
    "\n",
    "    all_urls = set()\n",
    "    article_urls= []\n",
    "    ems = []\n",
    "\n",
    "    ## URLs where we can also extract the paragraph text\n",
    "    article_paragraphs = article_text.find_all(\"p\")\n",
    "    for p in article_paragraphs:\n",
    "        if len(p.text.split()) < 10:\n",
    "            continue\n",
    "\n",
    "        for url in p.find_all(\"a\"):\n",
    "            try:\n",
    "                extract_res = tldextract.extract(url['href'])\n",
    "                if len(extract_res.domain) and len(extract_res.suffix):\n",
    "                    url_val = \".\".join([extract_res.domain,extract_res.suffix])\n",
    "                    if not len(bad_url_re.findall(url['href'])):\n",
    "                        article_urls.append((url.get_text(),url_val,url['href'],url.parent.name,url.parent.parent.name,p.text))\n",
    "                        all_urls.add(url['href'])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if not p.parent.has_attr(\"class\") or 'article-sources-box' not in p.parent['class']:\n",
    "            for emph in p.find_all(\"em\"):\n",
    "                ems.append((emph.get_text(),p.text))\n",
    "\n",
    "    # get URLs not attached to particular paragraphs\n",
    "    for url in article_text.find_all(\"a\"):\n",
    "        if not url.has_attr(\"href\"):\n",
    "            continue\n",
    "        if url['href'] in all_urls:\n",
    "            continue\n",
    "        try:\n",
    "            extract_res = tldextract.extract(url['href'])\n",
    "            if len(extract_res.domain) and len(extract_res.suffix):\n",
    "                url_val = \".\".join([extract_res.domain,extract_res.suffix])\n",
    "                if not len(bad_url_re.findall(url['href'])):\n",
    "                    article_urls.append((url.get_text(),url_val,url['href'],url.parent.name,url.parent.parent.name,''))\n",
    "                    urls= True\n",
    "        except:\n",
    "            print 'url fail'\n",
    "            continue\n",
    "\n",
    "    #############  CLAIM STUFF ########################\n",
    "    claim = None\n",
    "    result = None\n",
    "    \n",
    "    if not soup.find(\"p\",{\"itemprop\":\"claimReviewed\"}):\n",
    "        article_paragraphs = article_text.find_all(\"p\")\n",
    "        for p_it, paragraph in enumerate(article_paragraphs): \n",
    "            for span in paragraph.find_all(\"span\"):\n",
    "                if span.get_text() == \"Claim\":\n",
    "                    claim = paragraph.get_text().replace(\"Claim: \", \"\").strip()\n",
    "                    if len(article_paragraphs) > p_it + 1:\n",
    "                        result = article_paragraphs[p_it+1].get_text().strip()\n",
    "                        break\n",
    "                elif span.get_text() == \"Claim:\":\n",
    "                    claim = paragraph.get_text().replace(\"Claim:\", \"\").strip()\n",
    "                    result_obj = article_text.find(\"div\", {\"class\": \"claim-old\"})\n",
    "                    if result_obj:\n",
    "                        result = result_obj.get_text()\n",
    "                    elif len(article_paragraphs) > p_it + 1:\n",
    "                        result = article_paragraphs[p_it+1].get_text().strip()\n",
    "                break\n",
    "\n",
    "            for strong in paragraph.find_all(\"strong\"):\n",
    "                if strong.get_text().startswith(\"Claim\"):\n",
    "                    claim = paragraph.get_text().replace(\"Claim: \", \"\").strip()\n",
    "                    if len(article_paragraphs) > p_it + 1:\n",
    "                        result = article_paragraphs[p_it+1].get_text()\n",
    "                    break\n",
    "\n",
    "            for font in paragraph.find_all(\"font\"):\n",
    "                if font.get_text().startswith(\"Claim\"):\n",
    "                    claim = paragraph.get_text().replace(\"Claim: \", \"\").strip()\n",
    "\n",
    "                    if len(article_paragraphs) > p_it + 1 and \"Status\" in article_paragraphs[p_it+1].get_text():\n",
    "                            result = article_paragraphs[p_it+1].get_text().replace(\"Status: \",\"\").strip()\n",
    "                    else:\n",
    "                        for tab in article_text.find_all(\"table\"):\n",
    "                            tab_fonts = tab.find_all(\"font\")\n",
    "                            if tab_fonts and len(tab_fonts):\n",
    "                                result = tab_fonts[0].get_text()\n",
    "                    break\n",
    "\n",
    "            if claim:\n",
    "                break\n",
    "    else:\n",
    "        claim = soup.find(\"p\",{\"itemprop\":\"claimReviewed\"}).get_text().strip()\n",
    "        result = soup.find(\"div\",{\"class\":\"claim\"}).get_text().strip()\n",
    "\n",
    "    categories = []\n",
    "    try:\n",
    "        categories = [x.text for x in soup.find(\"div\",{\"class\": \"breadcrumb-nav\"}).find_all(\"a\")]\n",
    "    except:\n",
    "        print 'category_fail'\n",
    "        \n",
    "    if not result:\n",
    "        #print 'No result!! ', filename\n",
    "        return {\"article_url\": filename,\n",
    "                \"parse_result\" : \"no_result\"}\n",
    "    \n",
    "    return {\"article_url\": filename,\n",
    "            'categories' : categories,\n",
    "            \"claim\":claim,\n",
    "            \"result\" : result,\n",
    "            \"urls\" : article_urls,\n",
    "            'ems' : ems,\n",
    "            \"parse_result\" : \"ok\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Article Parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1750\n",
      "500\n",
      "2250\n",
      "1000\n",
      "1500\n",
      "250\n",
      "2000\n",
      "750\n",
      "2500\n",
      "1250\n",
      "3000\n",
      "4750\n",
      "3500\n",
      "4000\n",
      "2750\n",
      "4500\n",
      "3250\n",
      "5000\n",
      "3750\n",
      "4250\n",
      "6000\n",
      "6500\n",
      "5250\n",
      "7000\n",
      "5750\n",
      "7500\n",
      "6250\n",
      "6750\n",
      "5500\n",
      "7250\n",
      "9000\n",
      "7750\n",
      "9500\n",
      "8250\n",
      "10000\n",
      "8750\n",
      "9250\n",
      "8000\n",
      "9750\n",
      "8500\n",
      "10250\n"
     ]
    }
   ],
   "source": [
    "p = Pool(6)\n",
    "results = p.map(parse_article, enumerate(glob.glob(\"snopes_data/article_htmls/*\")))\n",
    "\n",
    "success_articles = []\n",
    "failed_articles = []\n",
    "\n",
    "for a in results:\n",
    "    if a['parse_result'] == 'ok':\n",
    "        success_articles.append(a)\n",
    "    else:\n",
    "        failed_articles.append(a)\n",
    "        \n",
    "p.close()\n",
    "#p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Archive.is Links\n",
    "\n",
    "***Note: Some of these archived links may no longer be available. This data is saved in*** ```snopes_data/archive_dot_is_mappings.txt``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N archived:  1135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "archived_urls = []\n",
    "url_res_from_archive = []\n",
    "failed_urls = []\n",
    "for article in success_articles:\n",
    "    url_set = set()\n",
    "    for url in article['urls']:\n",
    "        if 'archive.is' == url[1]:\n",
    "            archived_urls.append(url[2])\n",
    "print 'N archived: ', len(archived_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "for i,archived_link in enumerate(archived_urls):\n",
    "    if i % 50 == 0:\n",
    "        print i\n",
    "    if 'web.' in archived_link:\n",
    "        continue\n",
    "    page_html = requests.get(archived_link)\n",
    "    soup = BeautifulSoup(page_html.text,\"lxml\")\n",
    "    v = soup.find(\"input\",{\"name\":\"q\"})\n",
    "    if v:\n",
    "        url_res_from_archive.append((archived_link,v.get('value','')))\n",
    "    else:\n",
    "        failed_urls.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand t.co links\n",
    "\n",
    "\n",
    "***Note: Some of these archived links may no longer be available. This data is saved in*** ```snopes_data/t_dot_co_mappings.tsv``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N archived:  1304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "archived_tco_urls = []\n",
    "for article in success_articles:\n",
    "    url_set = set()\n",
    "    for url in article['urls']:\n",
    "        if 't.co' == url[1]:\n",
    "            archived_tco_urls.append(url[2])\n",
    "print 'N archived: ', len(archived_tco_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kennyjoseph/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n",
      "/Users/kennyjoseph/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n",
      "/Users/kennyjoseph/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n",
      "/Users/kennyjoseph/anaconda/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300\n"
     ]
    }
   ],
   "source": [
    "from grabURL import safeGetURLAndMetadata\n",
    "from time import sleep\n",
    "fail = []\n",
    "tco_expanded = []\n",
    "for i, a in enumerate(archived_tco_urls):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "    tco_expanded.append(safeGetURLAndMetadata(a, fastOnlyExpandURL=True))\n",
    "    #sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "of = io.open(\"snopes_data/archive_dot_is_mappings.txt\",\"w\")\n",
    "for u in url_res_from_archive:\n",
    "    of.write(u[0] + u\"\\t\" + u[1] + u\"\\n\")\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "of = io.open(\"snopes_data/t_dot_co_mappings.tsv\",\"w\")\n",
    "for u in tco_expanded:\n",
    "    if 'error' not in u:\n",
    "        of.write(u['initial_url'] + u\"\\t\" + u['canonical_url'] + u\"\\n\")\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(success_articles, open(\"snopes_data/success_articles.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
