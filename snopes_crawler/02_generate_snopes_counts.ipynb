{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from time import sleep\n",
    "import io\n",
    "import tldextract\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "success_articles = pickle.load(open(\"snopes_data/success_articles.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archive_dot_is_mappings = {x.split(\"\\t\")[0] : x.strip().split(\"\\t\")[1] for x in io.open(\"snopes_data/archive_dot_is_mappings.txt\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_dot_co_mappings = {x.split(\"\\t\")[0] : x.strip().split(\"\\t\")[1] for x in io.open(\"snopes_data/t_dot_co_mappings.tsv\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bs = set(['blogspot.com',\"wordpress.com\",'go.com','yahoo.com','googleusercontent.com','blogspot.nl','tumblr.com'])\n",
    "def construct_url(u):\n",
    "    full_url = u[2]\n",
    "    if u[1] == 'twitter.com':\n",
    "        full_url = full_url.replace(\"https://\",\"\")\n",
    "        full_url = full_url.replace(\"?src=hash\",\"\").lower()\n",
    "        x = full_url.rfind(\"/status/\")\n",
    "        if x > 0:\n",
    "            return full_url[:full_url.rfind(\"/status/\")]\n",
    "        if len(full_url) > 50:\n",
    "            return u[1]\n",
    "        return full_url\n",
    "    \n",
    "    if u[1] == 'archive.is':\n",
    "        if u[2] in archive_dot_is_mappings:\n",
    "            url = archive_dot_is_mappings[u[2]]\n",
    "            return construct_url(('',extract_base_url(url),url))\n",
    "        return u[1]\n",
    "    \n",
    "    if u[1] == 't.co':\n",
    "        if u[2] in t_dot_co_mappings:\n",
    "            url = t_dot_co_mappings[u[2]]\n",
    "            return construct_url(('',extract_base_url(url),url))\n",
    "        return u[1]\n",
    "    \n",
    "    if u[1] == 'archive.org':\n",
    "        fin = re.search(\"web/[0-9]+/\",u[2])\n",
    "        if fin:\n",
    "            return u[2][fin.end():]\n",
    "        return u[1]\n",
    "    \n",
    "    if u[1] == 'facebook.com':\n",
    "        full_url = \"/\".join(u[2].split(\"/\")[2:4])\n",
    "        if len(full_url) > 50:\n",
    "            return u[1]\n",
    "        return full_url.replace(\"www.\",\"\")\n",
    "    \n",
    "    if u[1] == 'reddit.com':\n",
    "        return \"/\".join(u[2].split(\"/\")[2:5]).replace(\"www.\",\"\")\n",
    "    \n",
    "\n",
    "    if \"webcache.googleusercontent.com\" in u[2]:\n",
    "        out = u[2][re.search(\"q=cache:[A-Za-z0-9\\-_]+:\",u[2]).end():]\n",
    "        if out.startswith(\"//\"):\n",
    "            out = out[2:]\n",
    "        return construct_url(('',extract_base_url(out),out))\n",
    "\n",
    "    if u[1] in bs:\n",
    "        return \"/\".join(u[2].split(\"/\")[2:3]).replace(\"www.\",\"\")\n",
    "    \n",
    "    return u[1]\n",
    "\n",
    "\n",
    "def extract_base_url(u):\n",
    "    extract_res = tldextract.extract(u)\n",
    "    if len(extract_res.domain) and len(extract_res.suffix):\n",
    "        return \".\".join([extract_res.domain,extract_res.suffix])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "false_re = re.compile('(false|incorrect|inaccurate|unproven)',re.I|re.U)\n",
    "true_re = re.compile(\"(true)\",re.I|re.U)\n",
    "good_cat = {\"Political News\", \"Politics\",\"Fact Check\",\"Fake News\"}\n",
    "\n",
    "res = []\n",
    "\n",
    "for x in success_articles:\n",
    "    \n",
    "    # indicative of a failed parse\n",
    "    if len(x['result']) > 100:\n",
    "        continue\n",
    "        \n",
    "    # get truth value\n",
    "    is_false = len(false_re.findall( x['result'])) \n",
    "    is_true = len(true_re.findall(x['result']))\n",
    "    tv = \"none\"\n",
    "    if is_false and is_true:\n",
    "        tv = 'both'\n",
    "    elif is_false:\n",
    "        tv = 'false'\n",
    "    elif is_true:\n",
    "        tv = 'true'\n",
    "        \n",
    "    is_political = False\n",
    "    if  len(set(x['categories'])&good_cat):\n",
    "        is_political = True\n",
    "\n",
    "        \n",
    "    article_title = x['article_url'][(x['article_url'].rfind(\"/\")+1):]\n",
    "    for u in x['urls']:\n",
    "        # can't resolve the donotlink stuff, unfortunately\n",
    "        if u[1] == 'donotlink.com':\n",
    "            continue\n",
    "        is_article_tag = u[0] == 'article'\n",
    "        is_archive = u[1] == 'archive.is'\n",
    "        res_df = {\n",
    "                \"orig_url\":u[1].lower(),\n",
    "                \"anchor_text\":u[0].lower(),\n",
    "                \"article\":article_title.lower(),\n",
    "                \"paragraph\":u[-1].lower(),\n",
    "                \"full_url\":u[2].lower(),\n",
    "                \"clean_url\":construct_url(u).lower(),\n",
    "                \"parent\":u[3],\n",
    "                \"parent_parent\":u[4],\n",
    "                \"truth\":tv,\n",
    "                \"is_pol\":is_political,\n",
    "                'is_article_tag':is_article_tag,\n",
    "                'is_archive_dot_is':is_archive,\n",
    "                'categories':x['categories']\n",
    "            }\n",
    "        if is_archive:\n",
    "            res_df['full_archive_url'] = archive_dot_is_mappings.get(u[2],\"\")\n",
    "        res.append(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31085, 14)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_df.to_csv(\"snopes_fake_news.csv\",encoding=\"utf8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
